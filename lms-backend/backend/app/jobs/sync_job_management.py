"""
Module for managing data synchronization between source and target databases.
"""
import os
import sys
import time
import json
import threading
import asyncio
import logging
import traceback
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Optional

from apscheduler.schedulers.blocking import BlockingScheduler
from sqlalchemy import Table, MetaData, inspect, select, func
from sqlalchemy.dialects.postgresql import insert as pg_insert
from sqlalchemy.orm import Session
from sqlalchemy.exc import SQLAlchemyError, OperationalError
from sqlalchemy.engine import Engine

# Láº¥y Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i cá»§a thÆ° má»¥c cha
parent_directory = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
# ThÃªm thÆ° má»¥c cha vÃ o sys.path
sys.path.append(parent_directory)

from app.model.base import Base
from app.model.user import User
from app.model.model import Model
from app.model.chat import Chat
from app.model.group_user import GroupUser
from app.model.model_group import ModelGroup
from app.model.group import Group
from app.utils.telegram_bot import TelegramBot

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Constants
BATCH_SIZE = 150  # Adjusted batch size for stability
THREAD_POOL_SIZE = 8  # Number of parallel threads

# Lock for synchronizing access to last_id
lock = threading.Lock()
class DataSyncManager:
    """Class to manage data synchronization between source and target databases."""
    def __init__(
        self,
        source_engine: Engine,
        target_engine: Engine,
        bot: Optional[TelegramBot] = None,
        batch_size: int = BATCH_SIZE,
        thread_pool_size: int = THREAD_POOL_SIZE
    ) -> None:
        self.source_engine = source_engine
        self.target_engine = target_engine
        self.bot = bot
        self.batch_size = batch_size
        self.thread_pool_size = thread_pool_size
        self.lock = threading.Lock()    
    def ensure_table_exists(self, base, table_name: str):
        """Ensure table exists in the target database."""
        inspector = inspect(self.target_engine)
        if table_name not in inspector.get_table_names():
            logging.info("Table '%s' not found. Creating...", table_name)
            base.metadata.create_all(self.target_engine, tables=[base.metadata.tables[table_name]])
            logging.info("Table '%s' created successfully.", table_name)
        else:
            logging.info("Table '%s' already exists.", table_name)
    def fetch_batch_data(self ,session: Session, table: Table, last_id: str, delta_time_epoch: Optional[int] = None) -> List[Dict]:
        """Fetch a batch of data from the source table."""
        query = (
            select(table).
            where(table.c.id > str(last_id)).
            order_by(table.c.id).
            limit(self.batch_size)
        )
        if delta_time_epoch:
            query = query.where(table.c.updated_at >= int(delta_time_epoch))
        result = session.execute(query).fetchall()  # Execute query and fetch the data
        return result

    # Fetch a batch of data from the source table
    # def fetch_batch_data_all(self ,session: Session, table: Table, last_id: str, batch_size: int) -> List[Dict]:
    #     """Fetch a batch of data from the source table when data is empty."""
    #     query = select(table).where(table.c.id > str(last_id)).order_by(table.c.id).limit(self.batch_size)
    #     return session.execute(query).fetchall()
    
    def fetch_batch_data_all(self, session: Session, table: Table, last_key: tuple, batch_size: int) -> List[Dict]:
        """Fetch a batch of data from the source table when data is empty, supporting composite keys."""
        pk_columns = list(table.primary_key.columns)  # Láº¥y danh sÃ¡ch cá»™t khÃ³a chÃ­nh
        if len(pk_columns) == 1:  # TrÆ°á»ng há»£p khÃ³a chÃ­nh Ä‘Æ¡n
            pk_col = pk_columns[0]
            query = (
                select(table)
                .where(pk_col > str(last_key[0]))
                .order_by(pk_col)
                .limit(self.batch_size)
            )
        else:  # TrÆ°á»ng há»£p khÃ³a chÃ­nh composite (nhÆ° model_group)
            model_id_col, group_id_col = pk_columns  # Giáº£ sá»­ thá»© tá»± lÃ  model_id, group_id
            last_model_id, last_group_id = last_key
            query = (
                select(table)
                .where(
                    (model_id_col > last_model_id) |
                    ((model_id_col == last_model_id) & (group_id_col > last_group_id))
                )
                .order_by(model_id_col, group_id_col)
                .limit(self.batch_size)
            )
        return session.execute(query).fetchall()

    # Perform batch upserts into the target table
    def insert_or_update_data(self ,session: Session, table: Table, data: List[Dict]) -> None:
        """Perform batch upserts into the target table."""
        if not data:
            return 
        try:
            # Danh sÃ¡ch cÃ¡c cá»™t trong báº£ng Ä‘Ã­ch
            target_columns = {col.name for col in table.columns}
            # Chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u
            transformed_data = []
            for row in data:
                transformed_row = {}
                for col in table.columns:
                    if col.name in row and col.name in target_columns:
                        value = row[col.name]
                        # Chuyá»ƒn Ä‘á»•i cÃ¡c cá»™t JSONField sang chuá»—i JSON
                        if isinstance(value, dict):
                            value = json.dumps(value)
                        transformed_row[col.name] = value
                transformed_data.append(transformed_row)
            stmt = pg_insert(table).values(transformed_data)
            upsert_stmt = stmt.on_conflict_do_update(
                index_elements=["id"],
                set_={col: stmt.excluded[col] for col in target_columns if col != "id"})
            result = session.execute(upsert_stmt)
            session.commit()
            logging.info(f"Batch processed: {len(transformed_data)} rows inserted/updated.")
            # Log sá»‘ lÆ°á»£ng báº£n ghi Ä‘Æ°á»£c thÃªm hoáº·c cáº­p nháº­t
            added = result.rowcount
            logging.info(f"Batch processed: {added} rows inserted/updated.")
            # Gá»­i thÃ´ng bÃ¡o qua bot náº¿u cÃ³
            if self.bot:
                self.bot.add_message(f"âœ… ÄÃ£ xá»­ lÃ½ {added} báº£n ghi trong batch nÃ y.")
        except Exception as e:
            session.rollback()
            logging.error(f"Error during upsert: {e}")
            # Optional: Send failure notification to bot
            if self.bot:
                self.bot.add_message(f"âŒ Xáº£y ra lá»—i khi xá»­ lÃ½ batch: {e}")

    def process_batch(self, source_session: Session, target_session: Session, source_table: Table, target_table: Table, last_id: str, delta_time_epoch: int, max_retries: int = 3, bot: Optional[TelegramBot] = None) -> Optional[str]:
        """Process batch"""
        logging.info(f"Fetching batch starting from ID: {last_id}")
        batch_data = self.fetch_batch_data(source_session, source_table, last_id, delta_time_epoch)
        if not batch_data:
            logging.info(f"No more data to fetch after ID: {last_id}.")
            if self.bot:
                self.bot.add_message(f"âœ… KhÃ´ng cÃ³ dá»¯ liá»‡u má»›i cáº§n Ä‘á»“ng bá»™ sau ID: {last_id}.")
            return None
        logging.info(f"Fetched {len(batch_data)} rows in the batch.")
        target_columns = {col.name for col in target_table.columns}
        transformed_data = [
            {
                col.name: (
                    json.dumps(row._mapping[col.name]) 
                    if col.name in row._mapping and isinstance(row._mapping[col.name], dict) 
                    else row._mapping[col.name] 
                    if col.name in row._mapping 
                    else None
                )
                for col in source_table.columns
                if col.name in target_columns
            }
            for row in batch_data
        ]

        logging.info(f"Transformed {len(transformed_data)} rows for insertion/update.")

        if self.bot:
            self.bot.add_message(f"ðŸ”„ ÄÃ£ chuyá»ƒn Ä‘á»•i {len(transformed_data)} dÃ²ng dá»¯ liá»‡u Ä‘á»ƒ chÃ¨n/cáº­p nháº­t.")

        self.insert_or_update_data(target_session, target_table, transformed_data)
        logging.info(f"Inserted/updated {len(transformed_data)} rows in the target table.")

        if bot:
            bot.add_message(f"âœ… ÄÃ£ chÃ¨n/cáº­p nháº­t {len(transformed_data)} dÃ²ng dá»¯ liá»‡u vÃ o báº£ng `{target_table.name}`.")

        last_row = batch_data[-1]  
        last_id = str(last_row._mapping["id"])

        logging.info(f"Processed batch up to ID: {last_id}.")
        return last_id

    # Synchronize data between source and target tables using parallel processing
    def process_sync(
        self,
        source_session: Session,
        target_session: Session,
        source_table: Table,
        target_table: Table,
        delta_days: int,
        bot: Optional[TelegramBot] = None
) -> None:
        retry_count = int(os.getenv('RETRY_COUNT', 0))
        max_retries = int(os.getenv('MAX_RETRIES', 3))
        retry_interval = int(os.getenv('RETRY_INTERVAL', 30))

        while retry_count < max_retries:
            try:
                # Chuyá»ƒn datetime vá» BigInt (seconds)
                current_timestamp_ms = int(datetime.now().timestamp())  # UNIX timestamp dáº¡ng seconds
                delta_seconds = int(timedelta(days=delta_days).total_seconds())  # Láº¥y sá»‘ giÃ¢y

                # TÃ­nh delta_time
                delta_time = current_timestamp_ms - delta_seconds

                print("Datetime (ms):", current_timestamp_ms)  # Dáº¡ng BIGINT
                print("Timedelta (ms):", delta_seconds)  # Sá»‘ mili giÃ¢y
                print("Delta Time (ms):", delta_time)  # Káº¿t quáº£ dáº¡ng BIGINT

                delta_time_epoch = delta_time  # LÆ°u giÃ¡ trá»‹ dáº¡ng BIGINT
                print("delta_time_epoch:", delta_time_epoch)  # In ra timestamp dÆ°á»›i dáº¡ng sá»‘ nguyÃªn
                last_id = "0"
                logging.info(f"Starting sync with delta time: {delta_time}.")

                with ThreadPoolExecutor(max_workers=self.thread_pool_size) as executor:
                    futures = []
                    while True:
                        # Schedule batch processing in parallel
                        future = executor.submit(
                            self.process_batch,
                            source_session,
                            target_session,
                            source_table,
                            target_table,
                            last_id,
                            delta_time_epoch,
                            # bot  # Truyá»n bot vÃ o hÃ m process_batch
                        )
                        futures.append(future)

                        # Wait for any completed future
                        completed_futures = []
                        for completed_future in as_completed(futures):
                            try:
                                new_last_id = completed_future.result()
                                if not new_last_id:
                                    logging.info("No more data to process. Stopping sync.")
                                    # No more data to process
                                    for f in futures:
                                        f.cancel()
                                    return

                                # Update last_id with thread-safe lock
                                with self.lock:
                                    last_id = new_last_id

                            except Exception as e:
                                logging.error(f"Error in batch processing: {e}")
                            finally:
                                completed_futures.append(completed_future)

                        # Remove completed futures from the list
                        for completed_future in completed_futures:
                            futures.remove(completed_future)

                logging.info("Data synchronization completed.")
                return  # ThoÃ¡t khá»i vÃ²ng láº·p náº¿u Ä‘á»“ng bá»™ thÃ nh cÃ´ng

            except OperationalError as e:
                logging.error(f"Lá»—i káº¿t ná»‘i: {e}. Thá»­ láº¡i sau 45 giÃ¢y...")
                if self.bot:
                    self.bot.add_message(f"âš ï¸ Lá»—i káº¿t ná»‘i Ä‘áº¿n DB. Thá»­ láº¡i sau 45 giÃ¢y...")
                source_session.rollback()
                target_session.rollback()
                retry_count += 1
                time.sleep(retry_interval)  # Chá» 45 giÃ¢y rá»“i thá»­ láº¡i

            except Exception as e:
                logging.error(f"Error during synchronization: {e}")
                source_session.rollback()
                target_session.rollback()
                

        logging.error("Max retries reached. Synchronization failed.")
        if self.bot:
            self.bot.add_message("âŒ ÄÃ£ háº¿t sá»‘ láº§n thá»­ láº¡i, Ä‘á»“ng bá»™ tháº¥t báº¡i.")

    # Check if target table is empty
    def is_table_empty(self ,session: Session, table: Table) -> bool:
            result = session.execute(select(func.count()).select_from(table)).scalar()
            return result == 0

    # Main synchronization function
    async def sync_data(self, table_name: str, delta_days: int = 1) -> None:
        retry_count = int(os.getenv('RETRY_COUNT', 0))
        max_retries = int(os.getenv('MAX_RETRIES', 3))
        retry_interval = int(os.getenv('RETRY_INTERVAL', 30))

        while retry_count < max_retries:
            source_session = Session(bind=self.source_engine)
            target_session = Session(bind=self.target_engine)

            start_time = datetime.now()
            if self.bot:
                self.bot.add_message(f"ðŸ”„ Äá»“ng bá»™ dá»¯ liá»‡u cho báº£ng `{table_name}` báº¯t Ä‘áº§u lÃºc {start_time.strftime('%Y-%m-%d %H:%M:%S')}.")

            try:
                logging.info("Starting data synchronization.")

                self.ensure_table_exists(Base, table_name)

                # Initialize MetaData
                source_metadata = MetaData()
                target_metadata = MetaData()

                # Bind engines to metadata
                source_metadata.bind = self.source_engine
                target_metadata.bind = self.target_engine

                source_table = Table(table_name, source_metadata, autoload_with=self.source_engine)
                target_table = Table(table_name, target_metadata, autoload_with=self.target_engine)

                total_records = 0
                if self.is_table_empty(target_session, target_table):
                    logging.info("Target table is empty. Transferring all data from source table.")
                    last_id = 0

                    while True:
                        batch_data = self.fetch_batch_data_all(source_session, source_table, last_id, self.batch_size)
                        if not batch_data:
                            break

                        # Láº¥y danh sÃ¡ch cá»™t cá»§a báº£ng Ä‘Ã­ch
                        target_columns = {col.name for col in target_table.columns}

                        # Chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u
                        transformed_data = [
                            {
                                col.name: (
                                    # Náº¿u giÃ¡ trá»‹ lÃ  dict, chuyá»ƒn Ä‘á»•i thÃ nh chuá»—i JSON
                                    json.dumps(row._mapping[col.name])
                                    if col.name in row._mapping and isinstance(row._mapping[col.name], dict)
                                    else row._mapping[col.name]
                                    if col.name in row._mapping
                                    else None
                                )
                                for col in source_table.columns
                                if col.name in target_columns  # Chá»‰ láº¥y cÃ¡c cá»™t cÃ³ trong báº£ng Ä‘Ã­ch
                            }
                            for row in batch_data
                        ]

                        self.insert_or_update_data(target_session, target_table, transformed_data)
                        total_records += len(batch_data)
                        last_row = batch_data[-1]  # Get the last row in the batch
                        last_id = str(last_row._mapping["id"])

                    logging.info("Initial data transfer completed.")
                else:
                    self.process_sync(source_session, target_session, source_table, target_table, delta_days, self.bot)

                end_time = datetime.now()
                duration = (end_time - start_time).total_seconds()

                if self.bot:
                    if total_records == 0:  # Náº¿u khÃ´ng cÃ³ báº£n ghi má»›i
                        self.bot.add_message(f"âœ… KhÃ´ng cÃ³ báº£n ghi má»›i Ä‘á»ƒ Ä‘á»“ng bá»™ cho báº£ng `{table_name}`.")
                    else:
                        self.bot.add_message(f"âœ… Tá»•ng cá»™ng {total_records} báº£n ghi Ä‘Æ°á»£c Ä‘á»“ng bá»™ cho báº£ng `{table_name}`.")
                    self.bot.add_message(f"âœ… Äá»“ng bá»™ báº£ng `{table_name}` hoÃ n thÃ nh lÃºc {end_time.strftime('%Y-%m-%d %H:%M:%S')} sau {duration:.2f} giÃ¢y.")

                logging.info("Data synchronization completed successfully.")
                return  # ThoÃ¡t khá»i vÃ²ng láº·p náº¿u Ä‘á»“ng bá»™ thÃ nh cÃ´ng

            except OperationalError as e:
                logging.error(f"Lá»—i káº¿t ná»‘i: {e}. Thá»­ láº¡i sau 30 giÃ¢y...")
                if self.bot:
                    self.bot.add_message(f"âš ï¸ Lá»—i káº¿t ná»‘i Ä‘áº¿n DB. Thá»­ láº¡i sau 30 giÃ¢y...")
                source_session.rollback()
                target_session.rollback()
                retry_count += 1
                await asyncio.sleep(retry_interval)  # Chá» 30 giÃ¢y rá»“i thá»­ láº¡i

            except SQLAlchemyError as e:
                error_message = f"âŒ Lá»—i cÆ¡ sá»Ÿ dá»¯ liá»‡u: {e}"
                if self.bot:
                    self.bot.add_message(error_message)
                logging.error(f"Database error: {e}")
                source_session.rollback()
                target_session.rollback()

            except Exception as e:
                # Láº¥y traceback chi tiáº¿t
                error_traceback = traceback.format_exc()

                # Táº¡o thÃ´ng bÃ¡o lá»—i
                error_message = f"âŒ Lá»—i nghiÃªm trá»ng trong quÃ¡ trÃ¬nh Ä‘á»“ng bá»™ báº£ng `{table_name}`: {e}\nTraceback:\n{error_traceback}"
                if self.bot:
                    self.bot.add_message(error_message)
                logging.error(f"Critical error: {e}\nTraceback:\n{error_traceback}")
                source_session.rollback()
                target_session.rollback()

            finally:
                source_session.close()
                target_session.close()
                logging.info("Database sessions closed.")
                if self.bot:
                    await self.bot.send_combined_message()

        logging.error("Max retries reached. Synchronization failed.")
        if self.bot:
            self.bot.add_message("âŒ ÄÃ£ háº¿t sá»‘ láº§n thá»­ láº¡i, Ä‘á»“ng bá»™ tháº¥t báº¡i.")

      
